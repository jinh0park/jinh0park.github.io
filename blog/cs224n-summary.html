<!DOCTYPE html><!--zp11YICo0no68g9G9pVjw--><html lang="ko"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/4cf2300e9c8272f7-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="preload" href="/_next/static/media/93f479601ee12b01-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/8cd908196c5f99fa.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-b66e35ffe30362a3.js"/><script src="/_next/static/chunks/4bd1b696-c023c6e3521b1417.js" async=""></script><script src="/_next/static/chunks/255-77a2a609117b40ff.js" async=""></script><script src="/_next/static/chunks/main-app-066a23ccca7f82af.js" async=""></script><script src="/_next/static/chunks/619-ba102abea3e3d0e4.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-7cf65f3911bce8b7.js" async=""></script><meta name="next-size-adjust" content=""/><title>강의 요약 - CS224n: Natural Language Processing with Deep Learning | My Velite Blog</title><meta property="og:title" content="강의 요약 - CS224n: Natural Language Processing with Deep Learning"/><meta property="og:type" content="article"/><meta property="article:published_time" content="2022-08-27T00:00:00.000Z"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="강의 요약 - CS224n: Natural Language Processing with Deep Learning"/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__variable_188709 __variable_9a8899 antialiased"><div hidden=""><!--$--><!--/$--></div><div class="flex flex-col min-h-screen"><header class="bg-gray-100 dark:bg-gray-900 border-b"><div class="container mx-auto px-4 py-4 flex justify-between items-center"><a class="text-xl font-bold hover:text-blue-600" href="/">jinh0park = jin + h0 + park</a><nav><a href="https://github.com/jinh0park" target="_blank" rel="noopener noreferrer" class="text-gray-600 dark:text-gray-300 hover:text-black dark:hover:text-white">GitHub</a></nav></div></header><main class="flex-grow container mx-auto px-4 py-8"><article class="prose dark:prose-invert mx-auto py-8"><h1 class="text-4xl font-bold">강의 요약 - CS224n: Natural Language Processing with Deep Learning</h1><div class="flex items-center gap-4 mt-2 mb-2"><p class="text-gray-500 !my-0">2022년 8월 27일</p><a href="/categories/dev"><span class="bg-gray-200 text-gray-800 text-sm font-medium px-3 py-1 rounded-full !my-0 hover:bg-gray-300">dev</span></a></div><hr class="!my-4"/><div><h1>Lecture 1 : Introduction and Word Vectors ~ Lecture 2 : Word Vectors 2 and Word Window Classification</h1>
<h2>I. Contents</h2>
<ul>
<li>Human language and word meaning
<ul>
<li>How do we represent the meaning of a word?</li>
<li>Word2Vec</li>
<li>GloVe</li>
</ul>
</li>
<li>Summary</li>
</ul>
<h2>II. Human language and word meaning</h2>
<h3>1. How do we represent the meaning of a word?</h3>
<h4>(1) Definition : <strong>meaning</strong></h4>
<p>Webster 영영사전 : the idea that is represented by a word, phrase, etc.</p>
<p>표준국어대사전("의미") : 말이나 글의 뜻</p>
<h4>(2) Commonest linguistic way of thinking of meaning :</h4>
<p>\[signifier(symbol) ↔ signified(idea or thing)\]</p>
<h4>(3) Common NLP solution</h4>
<p><em>WordNet</em>, a thesaurus containing lists of synonym sets and hypernyms</p>
<p>동의어, 유의어(synonyms) 또는 상의어(hypernyms)로 이루어진 사전을 통해 의미를 정의할 수 있다.</p>
<p>※ WordNet의 문제점</p>
<ul>
<li>뉘앙스를 담을 수 없다.</li>
<li>단어의 새로운 의미를 담을 수 없다.</li>
<li>동의어 등을 판단하는 기준이 주관적이다.</li>
<li>제작하는데 인력이 많이 소모된다.</li>
<li><strong>Word Similarity를 계산할 수 없다.</strong></li>
</ul>
<h4>(4) Representing words as discrete symbols</h4>
<p>"One-hot vector" representing :
\[motel = [0, 0, 0, 0, 0, 1, 0] \ hotel = [0, 0, 0, 1, 0, 0, 0]\]</p>
<p>※ One-hot vector representing의 문제점
모든 vector들이 orthogonal하기 때문에, similarity를 계산할 수 없다.</p>
<h4>(5) Representing words by their context</h4>
<p><strong>Distributional semantics</strong> : A word's meaning is given by the words that frequency appear close-by.
When a word \(w\) appears in a textm its context is the set of words that appears nearby.</p>
<p>특정 단어의 의미는, 글에서 그 특정 단어 주변에 어떤 단어들(=context)이 주로 오는지에 따라 파악할 수 있다.
위 아이디어를 이용하여, 단어를 Vectorize 할 수 있다.</p>
<p>Note : Word vectors are also called word embeddings or word representations, They are a <strong>distributed representation</strong>.</p>
<h4>(6) Conclusion</h4>
<p>NLP에서 단어를 표현하는 방식으로 WordNet, discrete representations, distributed representation 등이 있다.
이때 NLP에서 "유용한" 방식으로 단어를 표현하기 위해서는, 단어 간 유사도(Similarity)를 계산할 수 있도록 distributed representation이 가장 적절한 해법이다.
Distributional semantics에서 착안하여, 단어들을 실수 벡터로 표현하는 방법으로 Word2Vec을 알아본다.</p>
<h3>2. Word2Vec</h3>
<h4>(1) What is Word2Vec?</h4>
<p><strong>Word2Vec</strong> (Mikolov et al. 2013, <a href="http://arxiv.org/pdf/1301.3781.pdf">PDF</a>) is a framework for learning word vectors.</p>
<figure class="rehype-figure"><img src="/static/1-6eb1b2d1.PNG" alt=""></figure>
<p>\[Likelihood = L(\theta) = \prod_{t=1}^{T} \prod_{-m \le j\le m, j\ne0} P(w_{t+j}|w_t;\theta)\]
\[objective function = J(\theta) = -\frac{1}{T}log(L(\theta))\]</p>
<p>특정 단어에 대하여, 주변(Window)에 다른 단어들이 나타날 확률의 곱 (첫번째 \(\prod\))을
모든 단어마다 계산해 곱해주면 (두번째 \(\prod\)), 이를 Likelihood라고 하며,
-Log를 취하여 objective function을 최소화 함으로써 word representation을 찾을 수 있다.</p>
<h5>Likelihood란?</h5>
<p>관측 데이터 \(x\)가 있을 때, 어떤 분포 \(\theta\)를 주고, 그 분포에서 데이터가 나왔울 확률을 말한다.
\[L(\theta|x)=P_{\theta}(X=x)=\prod_{k=1}^n P(x_k|\theta)\]
Word2Vec에서 \(\theta\)란 "to be defined in terms of our vectors", 즉 optimze할 word vector들의 값을 의미하며(이에 따라 분포가 정해진다),
위 Likelihood의 정의로부터 본다면, \(w_t\)마다 likelihood를 구한 후 T번 곱하는 것이라고 볼 수 있겠다.</p>
<h4>(2) How to calculate the probability?</h4>
<p>\[P(o|c) = \frac{exp(u_o^Tv_c)}{\sum_{w\in V}exp(u_w^Tv_c)}\]
This is an example of the <strong>softmax function</strong>.
The softmax fuction maps arbitrary values \(x_i\) to a probability distribution \(p_i\)
("max" because amplifies probability to smaller \(x_i\), "soft" because still assigns some probability to smaller \(x_i\))</p>
<h4>(3) Optimization</h4>
<p>Gradient descent, chain rule, SGD ... (생략)</p>
<h4>(4) Word2Vec : More details</h4>
<ol>
<li>Two model variants</li>
</ol>
<ul>
<li>Skip-grams (SG) ☞ 위에서 한 내용</li>
<li>Continuous Bag of Words (CBOW)</li>
</ul>
<ol start="2">
<li>Additional efficiency in Training</li>
</ol>
<ul>
<li>Naive softmax</li>
<li>Negative samlping</li>
</ul>
<h4>(5) The skip-gram model with negative sampling <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">PDF</a></h4>
<p>Main idea : train binary logistic regressions for a true pair(center word and a word in its context window) versus several noise pairs (the center word paired with a random word)</p>
<p>Likelihood를 계산하는 식 중, softmax는 computationally expensive하므로(vocabulary에 있는 모든 단어에 대해 내적 필요), 이를 Negative-sampling을 이용한 방식으로 대체한다.</p>
<h3>3. GloVe</h3>
<h4>(1) Co-occurrence matrix K</h4>
<figure class="rehype-figure"><img src="/static/2-e6e5c360.PNG" alt=""></figure>
<ul>
<li>Window length 1 (most common : 5-10)</li>
<li>["I like deep learning", "I like NLP", "I enjoy flying"]</li>
</ul>
<p>Co-occurrence matrix는 V * V 크기의 행렬이고, sparsity issue가 있으므로 차원을 낮추는 과정이 필요하며, 대표적으로 SVD(Singular Value Decomposition)이 있다.</p>
<p>이때, raw counts에 SVD를 바로 적용하는 것은 잘 작동하지 않고, the, he, has와 같이 과도하게 자주 등장하는 단어들(function words)의 count를 clipping하거나, count에 log를 취하거나, function words를 무시하는 방법 등을 추가로 요한다.</p>
<h4>(2) GloVe</h4>
<p><a href="http://nlp.stanford.edu/pubs/glove.pdf">GloVe: Global Vectors for Word Representation</a></p>
<p>Q: How to we capture ratios of co-occurrence probabilities as linear meaning components in a word vector space?</p>
<p>A: Log-bilinear model (아래는 with vector differences)
\[w_i\cdot w_j=logP(i|j) \ w_x\cdot (w_a-w_b) = log {\frac{P(x|a)}{P(x|b)}}\]</p>
<p>\(w_i\cdot w_j=logP(i|j)\) 가 되도록 \(w\)를 optimize하는 것이 목표이다. \(P\)는 co-occurrence matrix로 부터 계산됨.</p>
<p>\[J = \sum^V_{i,j=1}f(X_{ij})(w_i^T\tilde w_j+b_i+\tilde b_j - logX_{ij})^2\]</p>
<p>\(f\) -> clipping function fot function words</p>
<ul>
<li>Fast training</li>
<li>Scalable to huge corpora</li>
<li>Good performance even with small corpus and small vectors</li>
</ul>
<h4>(3) Evaluation of word vectors</h4>
<p>Related to general evaluation in NLP : Intrinsic vs. extrinsic</p>
<h5>Intrinsic word vector evaluation</h5>
<ol>
<li>Word vector analogies (ex. man : woman = king : ???, ??? = queen)</li>
<li>Word vector distances and their correlation with human judgements</li>
</ol>
<h5>Extrinsic word vector evaluation</h5>
<ol>
<li>Named entity recognition</li>
</ol>
<h4>(4) Word senses and word sense ambiguity</h4>
<p>Most words have lots of meanings!</p>
<figure class="rehype-figure"><img src="/static/3-6a47b122.PNG" alt=""></figure>
<ol>
<li><a href="http://www.aclweb.org/anthology/Q15-1016">Improving Distributional Similarity with Lessons Learned from Word Embeddings</a> Huang et al. 2012</li>
<li><a href="https://transacl.org/ojs/index.php/tacl/article/viewFile/1346/320">Linear Algebraic Structure of Word Senses, with Applications to Polysemy</a> Arora et al. 2018</li>
</ol>
<h1>III. Summary</h1>
<p>Human language를 컴퓨터가 이해할 수 있도록 표현하기 위해서는 "word vector"를 설계해야한다. 이를 위한 시도들로 Word2Vec, GloVe를 알아보았다. Word2Vec은 SG 또는 CBOW로 구현할 수 있고, gradient descent를 효율적으로 하기 위해 negative sampling을 도입할 수 있다. GloVe는 co-occurrence matrix를 이용하여 word vector를 구한다.</p>
<p>Word vector를 만들었으면 이를 평가할 수 있어야 한다. NLP에서 평가 지표로 Intrinsic evaluation 또는 Extrinsic evaluation이 있다.</p>
<hr>
<h1>Lecture 3 : Backprop and Neural Networks ~ Lecture 4 : Dependency Parsing</h1>
<h2>I. Contents</h2>
<ul>
<li>BackProp and Neural Networks
<ul>
<li>Simple NER</li>
<li>Backpropagation</li>
</ul>
</li>
<li>Dependency Parsing
<ul>
<li>Syntactic Structure : Constituency and Dependency</li>
<li>Dependecy Grammar and Treebanks</li>
<li>Transition-based dependency parsing</li>
</ul>
</li>
<li>Summary</li>
</ul>
<h2>II. BackProp and Neural Networks</h2>
<ol>
<li>
<p>Simple NER : Window classification using binary logistic classifier</p>
</li>
<li>
<p>Backpropagation</p>
</li>
</ol>
<p>CS231n에서 배웠던 내용과 대부분 중복되므로, 이해하는데 유용한 슬라이드 몇 장만 아카이브.</p>
<div class="rehype-figure-container"><figure class="rehype-figure"><img src="/static/cs224n-2021-lecture03-neuralnets_38-b86ba0bd.jpg" alt=""></figure><figure class="rehype-figure"><img src="/static/cs224n-2021-lecture03-neuralnets_39-08e9b207.jpg" alt=""></figure><figure class="rehype-figure"><img src="/static/cs224n-2021-lecture03-neuralnets_40-c2e59963.jpg" alt=""></figure><figure class="rehype-figure"><img src="/static/cs224n-2021-lecture03-neuralnets_41-d8fbe093.jpg" alt=""></figure><figure class="rehype-figure"><img src="/static/cs224n-2021-lecture03-neuralnets_54-f2a1f476.jpg" alt=""></figure><figure class="rehype-figure"><img src="/static/cs224n-2021-lecture03-neuralnets_65-10582ea5.jpg" alt=""></figure><figure class="rehype-figure"><img src="/static/cs224n-2021-lecture03-neuralnets_72-f128ba3f.jpg" alt=""></figure><figure class="rehype-figure"><img src="/static/cs224n-2021-lecture03-neuralnets_73-7079b025.jpg" alt=""></figure></div>
<h2>III. Dependency Parsing</h2>
<h3>1. Syntactic Structure : Constituency and Dependency</h3>
<h4>(1) Constituency</h4>
<p>Phrase structure organizes words into nested constituents.</p>
<ul>
<li>Starting unit : words
<ul>
<li>the, cat, cuddly, by, door</li>
</ul>
</li>
<li>Words combine into phrases
<ul>
<li>the cuddly cat, by the door</li>
</ul>
</li>
<li>Phrases can combine into bigger phrases
<ul>
<li>the cuddly cat by the door</li>
</ul>
</li>
</ul>
<h4>(2) Dependency</h4>
<p>Dependency structure shows which words depend on (modify, attach to, or are arguments of) which other words.</p>
<h4>(3) Sentence structure</h4>
<p><strong>Why do we need sentence structure?</strong></p>
<p>Humans communicate complex ideas by composing words together into bigger units to convey complex meanings. Listeners need to work out what modifies what. A model needs to understand sentence structure in order to be able to interpret language correctly.</p>
<p><strong>Ambiguities</strong></p>
<ul>
<li>Prepositional phrase attachment ambiguity</li>
<li>Coordination scope ambiguity</li>
<li>Adjectival/Advebial modifier ambiguity</li>
<li>Verb phrase attachment ambiguity</li>
</ul>
<h3>2. Dependecy Grammar and Treebanks</h3>
<h4>(1) Dependecy Grammar</h4>
<p>Dependency syntax postulates that syntactic structure consists of relations between lexical itemns, normally binary asymmetric relations ("arrows") called <strong>dependencies</strong></p>
<figure class="rehype-figure"><img src="/static/4-fcdc4d4d.PNG" alt=""></figure>
<figure class="rehype-figure"><img src="/static/5-b67cd3de.PNG" alt=""></figure>
<p>We usually add a fake ROOT so every word is a dependent of precisely 1 other node.</p>
<h4>(2) Treebanks</h4>
<p>The rise of annotated data &#x26; Universal dependencies treebanks</p>
<figure class="rehype-figure"><img src="/static/6-1f8aa17e.PNG" alt=""></figure>
<p>A treebank gives us many things</p>
<ul>
<li>Reusability of the labor</li>
<li>Broad coverage, not just a few intuitions</li>
<li>Frequencies and distributional information</li>
<li>A way to evaluate NLP systems</li>
</ul>
<p>Sources of information for dependency parsing</p>
<ul>
<li>Bilexical affinities</li>
<li>Dependecy distances</li>
<li>Intervening material</li>
<li>Valency of heads</li>
</ul>
<h3>3. Transition-based dependency parsing</h3>
<h4>Basic transition-based dependency parser</h4>
<ul>
<li>Arc-standard transition-based parser
<ul>
<li>3 actions : SHIFT, LECT-ARC, RIGHT-ARC</li>
</ul>
</li>
<li>MaltParser
<ul>
<li>Each action is predicted by a discriminative classfier over each legal move.</li>
<li>It provides very fast linear time parsing, with high accuracy - great for parsing the web.</li>
</ul>
</li>
</ul>
<h3>4. Neural dependency parsing</h3>
<figure class="rehype-figure"><img src="/static/7-d0f6ad0c.PNG" alt=""></figure>
<p>Neural networds can accurately determine the structure of sentences, supporting interpretation.</p>
<h2>IV. Summary</h2>
<p>문장을 이해하기 위해서는 각 단어가 어떤 단어를 수식하고 있는지 그 관계를 파악해야한다. 이를 Dependency라고 하며, 주어진 문장에 대해 그 수식 관계를 파악하는 것을 Dependency parsing이라고 한다. 수많은 corpus에 대해 Dependency parsing을 하는 것은 NLP에서 중요한 과제 중 하나이다.</p>
<p>Transition-based dependency parsing은 Stack, Buffer 개념을 도입하여 알고리즘으로 Dependency parsing을 하는 기법이다. MaltParser은 머신러닝을 이용해 높은 성능을 보여주었으며, 비교적 최근 인공신경망을 이용한 Neural dependency parsing은 가장 강력한 기법 중 하나로 자리잡았다. 그 외에도 Graph-based dependency parser 등이 있다.</p>
<hr>
<h1>Lecture 5 : Recurrent Neural Networks and Language Models ~ Lecture 6 : Vanishing Gradients, Fancy RNNs, Seq2Seq</h1>
<h2>I. Contents</h2>
<ul>
<li>Neural Dependency Parsing
<ul>
<li>Neural Dependency Parsing</li>
<li>A bit more about neural Networks</li>
</ul>
</li>
<li>Language Modeling and RNNs
<ul>
<li>Language Modeling</li>
<li>N-grams Language Models</li>
<li>Neural Language Models</li>
<li>Evaluating Language Models</li>
</ul>
</li>
<li>LSTM : Long Short-Term Memory RNNs
<ul>
<li>Problems with Vanishing and Exploding Gradients</li>
<li>LSTMs</li>
<li>More about vanishing/exploding gradient Problem</li>
<li>Bidirectional and Multi-layer RNNs: Motivation</li>
</ul>
</li>
<li>Summary</li>
</ul>
<h2>II. Neural Dependency Parsing</h2>
<h3>1. Neural Dependency Parsing</h3>
<p>Deep learning classifiers are non-linear classifiers (cf. Traditional ML classifiers only give linear decision boundaries)</p>
<p><a href="https://www.emnlp2014.org/papers/pdf/EMNLP2014082.pdf">A Fast and Accurate Dependency Parser using Neural Networks</a> Chen and Manning, 2014</p>
<h3>2. A bit more about neural networks</h3>
<ul>
<li>Regularization</li>
<li>Dropout</li>
<li>Vectorization</li>
<li>Non-linearities</li>
<li>Parameter initialization</li>
<li>Optimizers</li>
<li>Learning rates</li>
</ul>
<h2>III. Language modeling and RNNs</h2>
<h3>1. Language Modeling</h3>
<p><strong>Language Modeling</strong> is the task of predicting what word comes next.</p>
<p>More formally: given a sequence of words \(x^{(1)},x^{(2)},\cdots,x^{(t)}\), compute the probability distribution of the next word \(x^{(t+1)}\)</p>
<p>\[P(x^{(t+1)}\mid x^{(t)},\cdots x^{(1)})\]</p>
<p>where \(x^{(t+1)}\) can be any word in the vocabulary \(V = {w_1, \cdots , w_{\mid V\mid }}\)</p>
<p>주어진 단어들의 sequence가 있을 때, 그 다음에 올 단어의 확률분포를 구하는 것을 Language modeling이라고 한다.</p>
<p>각 sequence step마다 "단어가 다음에 올 확률"을 곱하면 전체 텍스트의 확률 분포가 되며, 식은 아래와 같다.</p>
<p>\[P(x^{(1)},\cdots ,x^{(T)})=P(x^{(1)})\times P(x^{(2)\mid x^(1)}) \times \cdots \ = \prod_{t=1}^T P(x^{(t)}\mid x^{(t-1)},\cdots ,x^{(1)}) \]</p>
<h3>2. N-gram Language Models</h3>
<p>Idea : Collect statistics about how frequent different n-grams are and use these to predict next word.</p>
<p>First we make a <strong>Markov assumption</strong> : \(x^{(t+1)}\) depends only on the preceding \(n-1\) words</p>
<p>\[P(x^{(t+1)}\mid x^{(t)},\cdots x^{(1)}) = P(x^{(t+1)}\mid x^{(t)},\cdots x^{(t-n+2)})
\ = \frac {P(x^{(t+1)},x^{(t)},\cdots x^{(t-n+2)})} {P(x^{(t)},\cdots x^{(t-n+2)})} \]</p>
<p>\[\approx \frac {count(x^{(t+1)},x^{(t)},\cdots x^{(t-n+2)})} {count(x^{(t)},\cdots x^{(t-n+2)})}\]</p>
<h4>Problems of N-gram</h4>
<ul>
<li>Sparsity Problems
<ul>
<li>Problem 1 : 위 식에서 분자 부분의 count가 0이라면, 해당 단어의 확률이 0으로 고정됨 <br>
Solution : Add small \(\delta\) to the count for every \(w \in V\)</li>
<li>Problem 1 : 위 식에서 분모 부분의 count가 0이라면, 그 다음 단어의 확률을 정의할 수 없음 <br>
Solution : 마지막 단어 하나 생략하고 찾기</li>
</ul>
</li>
<li>Storage Problems
<ul>
<li>Need to store count for all n-grams you saw in the corpus.</li>
</ul>
</li>
</ul>
<p>Results : Surprisingly grammatical!</p>
<p>...but <strong>incoherent</strong>. We need to consider more than three words at a time if we want to model language well. But increasing n worsens <strong>sparsity problem</strong>, and increase model size.</p>
<h3>3. Neural Language Models</h3>
<h4>(1) A fixed-window neural Language Model</h4>
<p><a href="https://jmlr.org/papers/volume3/tmp/bengio03a.pdf">A Neural Probabilistic Language Model</a>, Y.Bengio, et al. (2000/2003)</p>
<figure class="rehype-figure"><img src="/static/8-69587cfc.png" alt=""></figure>
<h5>Improvements over n-gram LM</h5>
<ul>
<li>No sparsity Problem</li>
<li>Don't need to store all observed n-grams</li>
</ul>
<h5>Remaining Problems</h5>
<ul>
<li>Fixed window is too small</li>
<li>Enlarging window enlarges \(W\) ☞ Window can never be large enough!</li>
<li>No symmetry in how the inputs are processed</li>
</ul>
<p>☞ We need a neural architecture that can process any length input!</p>
<h4>(2) Recurrent Neural Networks</h4>
<p><strong>Core idea</strong> : Apply the same weights \(W\) repeatedly!</p>
<figure class="rehype-figure"><img src="/static/9-ce32deb1.png" alt=""></figure>
<h5>RNN Advantages</h5>
<ul>
<li>Can process <strong>any length</strong> input</li>
<li>Computation for step \(t\) can (in theory[<em>due to gradient vanishing problem, "in theory"</em>]) use information from many steps back</li>
<li><strong>Model size doesn't increase</strong> for longer input context</li>
<li>Same weights applied on every timestep, so there is <strong>symmetry</strong> in how inputs are processed.</li>
</ul>
<h5>RNN Disadvantages</h5>
<ul>
<li>Recurrent computation is slow (it runs in the for loop, can't be computed parallelly)</li>
<li>In practice, difficult to access information from many steps back</li>
</ul>
<h5>Training an RNN Language Models</h5>
<p>\[J^{(t)}(\theta)=CE(y^{(t)}, \hat y^{(t)})=-\sum_{w\in V}y_w^{(t)}=-log\hat y^{(t)}<em>{x</em>{t+1}}\]</p>
<p>\[J(\theta)=\frac {1}{T} \sum^T_{t=1}J^{(t)}(\theta)\]</p>
<h3>4. Evaluating Language Models</h3>
<p>The standard evaluation metric for LM is <strong>perplexity</strong></p>
<p>\[perplexity = \prod_{t=1}^T (\frac {1}{P_{LM}(x^{(t+1)}\mid x^{(t)},\cdots ,x^{(1)})})^{1/T}
\ = exp(J(\theta))\]</p>
<p><strong>Lower</strong> perplexity is better!</p>
<p>probability of corpus의 기하평균의 역, 모든 단어를 정확히 맞춘다면 \(perplexity = 1\)</p>
<h4>Why should we care about Language Modeling?</h4>
<ul>
<li>Language Modeling is a <strong>benchmark task</strong> that helps us <strong>measure our progress</strong> on understanding language</li>
<li>Language Modeling is a <strong>subcomponent</strong> of many NLP tasks</li>
</ul>
<h2>IV. LSTM : Long Short-Term Memory RNNs</h2>
<h3>1. Problems with Vanishing and Exploding Gradients</h3>
<h4>Vanishing gradients</h4>
<figure class="rehype-figure"><img src="/static/10-e51da064.png" alt=""></figure>
<h4>Exploding gradients</h4>
<p>Exploding gradients can solve by simple methods such as <strong>gradient clipping</strong> .</p>
<p>How about a RNN with separate memory to fix the <strong>vanishing</strong> gradient problem? ☞ <strong>LSTMs</strong></p>
<h3>2. LSTMs</h3>
<p><a href="http://www.bioinf.jku.at/publications/older/2604.pdf">Hochreiter and Schmidhuber</a>(1997)</p>
<p><a href="https://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf">Gers et al</a>(2000) -> Crucial part of the modern LSTM is here!</p>
<p>On step \(t\), there is a hidden state \(h^{(t)}\) and a cell state \(c^{(t)}\)</p>
<ul>
<li>Both are vectors length \(n\)</li>
<li>The cell stores <strong>long-term information</strong></li>
<li>The LSTM can read, erase, and write information from the cell
<ul>
<li>the cell becomes conceptually rather like RAM in a computer</li>
</ul>
</li>
</ul>
<figure class="rehype-figure"><img src="/static/11-f333b6f1.png" alt=""></figure>
<figure class="rehype-figure"><img src="/static/12-927dd4fb.png" alt=""></figure>
<p>LSTM이 Gradient vanishing을 해결할 수 있는 핵심 구조는 cell state이다. cell state는 곱연산이 아닌 <strong>합연산</strong>을 통해 다음 cell로 전해지므로, step을 오래 거치더라도 vanishing이 발생하지 않는다.</p>
<p>cell state에 long term memory가 저장되므로, hidden state를 계산할 때 output gate를 통해 long term memory에 저장된 정보를 얼마나 사용할지 결정할 수 있다.</p>
<h4>3. More about vanishing/exploding gradient Problem</h4>
<h5>Is vanishing/exploding gradient just a RNN problem?</h5>
<p>No! It can be a problem for all neural architectures, especially very deep ones</p>
<p>Solution : Add more direct connections (e.g. ResNet, DenseNet, HighwayNet, and etc.)</p>
<h4>4. Bidirectional and Multi-layer RNNs: Motivation</h4>
<h5>(1) Bidirectional RNNs</h5>
<p>문장 구조상, 뒤에 있는 단어들 까지 보아야 단어의 의미를 파악할 수 있는 경우가 있다. (e.g. <em>the movie was "teriibly" exciting!</em>, <em>terribly</em>가 긍정적인 의미로 사용되었음을 알기 위해서는 뒤의 <em>exciting</em>도 보아야 한다.)</p>
<figure class="rehype-figure"><img src="/static/13-d1bf78b6.png" alt=""></figure>
<p>\(h^{(t)}\) has the dimension of \(2d\) (\(d\) is the hidden size of FW or BW)</p>
<p>Note: Bidirectional RNNs are only applicable if you have access to the <strong>entire input sequence</strong> ☞ Not applicable to LM!</p>
<h5>(2) Multi-layer RNNs</h5>
<figure class="rehype-figure"><img src="/static/14-c758b546.png" alt=""></figure>
<h2>V. Summary</h2>
<p>Language Modeling은 자연어처리에서 benchmark test &#x26; subcomponent 역할을 하는 task이다. 딥러닝 이전에 N-grams LM이 존재하였으며, RNN을 도입하면서 성능이 비약적으로 상승하였다.</p>
<p>한편, RNN의 특성상 gradient vanishing(exploding) 문제가 발생하는데, 이를 해결하기 위해 RNNs중 하나로서 LSTMs이 도입되었다. LSTM의 핵심은 long term memory를 저장하는 cell state로, 합연산을 통해 값이 전달되므로 vanishing이 현저하게 줄어든다.</p>
<p>RNNs의 성능을 더욱 향상시키기 위한 시도로 Bidirectional, Multi-layer RNNs 등이 있으며, 오늘날 가장 좋은 성능을 보이는 모델 중 하나인 BERT와 같은 Transformer-based network에서도 이러한 구조들을 채택하고 있다.</p>
<hr>
<h1>Lecture 7 : Machine Translation, Attention, Subword Models</h1>
<h2>I. Contents</h2>
<ul>
<li>Machine translation</li>
<li>Seq2seq
<ul>
<li>Neural Machine Translation</li>
<li>Training a Neural Machine Translation System</li>
<li>Multi-layer RNNs</li>
<li>Decoding varieties</li>
<li>Evaluating Maching Translation</li>
</ul>
</li>
<li>Attention
<ul>
<li>Seq2seq: the bottleneck problem</li>
<li>Attention</li>
</ul>
</li>
</ul>
<h2>II. Machine Translation</h2>
<p>Machine Translation is the task of translating a sentence \(x\) from one language (<strong>the source language</strong>) to a sentence \(y\) in another language (<strong>the target language</strong>).</p>
<h3>1990s-2010s: Statistical Machine Translation</h3>
<figure class="rehype-figure"><img src="/static/15-28a58fc1.png" alt=""></figure>
<h4>Alignment</h4>
<ul>
<li>Q. How to learn translation model \(P(x\mid y)\)?
<ul>
<li>First, need large amount of parallel data e.g. <em>The Rosetta Stone</em></li>
<li>Break it down further: Introduce latent \(a\) variable into the model: \(P(x, a\mid y)\) where \(a\) is the <strong>alignment</strong>, i.e. word-level correspondence between source sentence \(x\) and target sentence \(y\)</li>
<li>Alignments are <strong>latent variables</strong>: They aren't explicitly specified in the data!</li>
</ul>
</li>
</ul>
<p>언어마다 문법이나 단어 체계가 다르기 때문에, 번역을 하기 위해서는 source sentence와 target sentence 의 단어가 각각 어떻게 대응되는지 파악해야하며, 이를 alignment라고 한다.</p>
<p>Alignment는 one-to-one, many-to-one, one-to-many, many-to-many 등 복잡하게 구성되며, dependency parsing에서의 arc처럼 명시적으로 특정되지 않고 SMT에 내장되므로 <strong>latent variable</strong>이라고 부른다.</p>
<h4>Decoding for SMT</h4>
<p>Enumerating every possible \(y\) and calculate the probability is too expensive.</p>
<p>Answer : Impose strong independence assumptions in model, use dynamic programming for globally optimal solutions</p>
<figure class="rehype-figure"><img src="/static/16-1a8ce14c.png" alt=""></figure>
<h4>Conclusion</h4>
<p>The best systems of SMT were "extremely complex"</p>
<h2>III. Seq2Seq</h2>
<h3>1. Neural Machine translation</h3>
<p><strong>Neural Machine Translation (NMT)</strong> is a way to do Machine Translation with a <em>single end-to-end neural network</em></p>
<p>The neural network architecture is called a <strong>sequence-to-sequence</strong> model (a.k.a. <strong>seq2seq</strong>) and it involves <strong>two RNNs</strong></p>
<figure class="rehype-figure"><img src="/static/17-b8287b63.png" alt=""></figure>
<ul>
<li>seq2seq is useful for <strong>more than just MT</strong>
<ul>
<li>Summarization</li>
<li>Dialogue</li>
<li>Parsing</li>
<li>Code generation</li>
</ul>
</li>
<li>seq2seq model is an example of a <strong>Conditional Language Model</strong></li>
</ul>
<h3>2. Training a Neural Machine Translation System</h3>
<p>\[J(\theta) = \frac {1}{T} \sum_{t=1}^T J_t\]
(\(J_t\) is negative lof prob of the word)</p>
<p>seqseq is optimized as a <strong>single system</strong>, Backpropagation operates "end-to-end"</p>
<h3>3. Multi-layer RNNs</h3>
<ul>
<li>High-performing RNNs are usually multi-layer : 2 to 4 layers!</li>
<li>Usually, skip-connections/dense-connections are needed to train deeper RNNs (e.g. 8 layers)</li>
<li>Transformer-based networks (e.g. BERT) are usually deeper, like 12 or 24 layers.</li>
</ul>
<h3>4. Decoding varieties</h3>
<h4>Greedy Decoding</h4>
<ul>
<li>Take most probable word on each step</li>
<li><strong>Greedy decoding has no way to undo decisions</strong></li>
</ul>
<h4>Exhaustive search Decoding</h4>
<ul>
<li>Ideally : We could tru computing all possible sequences \(y\) and find \(y\) that maximizes :
\[P(y\mid x)=\prod^T_{t=1}P(y_t\mid y_1,\cdots ,y_{t-1},x)\]</li>
<li><strong>This \(O(V^T)\) complexity is far too expensive!!!</strong></li>
</ul>
<h4>Beam search Decoding</h4>
<ul>
<li>Core idea : On each step of decoder, keep track of the <em>k most probable partial translations</em> (which we call <strong>hypotheses</strong>)</li>
<li>Beam search is not guaranteed to find optimal solution, but much more efficient than exhaustive search.</li>
</ul>
<h4>Advantages and Disadvantages of NMT</h4>
<h5>Advantages</h5>
<ul>
<li>Better performance</li>
<li>A single neural network to be optimized end-to-end
<ul>
<li>No subcomponents to be individually optimized</li>
</ul>
</li>
<li>Requires much less human engineering effort
<ul>
<li>No feature engineering</li>
<li>Same method for all language pairs</li>
</ul>
</li>
</ul>
<h5>Disadvantages</h5>
<p>Compared to SMT :</p>
<ul>
<li>Less interpretable</li>
<li>Difficult to control (e.g. can't easily specify rules or guidelines for translation)</li>
</ul>
<h3>5. Evaluating Maching Translation</h3>
<p><strong>BLEU</strong> (Bilingual Evaluation Understudy)</p>
<p>BLEU compares the machine-written translation to one or several human-written translation(s), and computes a similarity score based on :</p>
<ul>
<li>n-gram precision</li>
<li>Plus a penalty for too-short system translations</li>
</ul>
<p><a href="http://incredible.ai/nlp/2020/02/29/BLEU/">Learn More - Incredible.AI : BLEU</a></p>
<ul>
<li>BLEU is useful, but imperfect</li>
</ul>
<h2>IV. Attention</h2>
<h3>1. Seq2seq: the bottleneck problem</h3>
<p>The last hidden state of encoder which is fed to decoder <strong>needs to capture all information</strong> about toe source sentence. ☞ "Information Bottleneck!"</p>
<h3>2. Attention</h3>
<h4>Overview</h4>
<p><strong>Attention</strong> provides a solution to the bottleneck problem</p>
<p>Core idea: on each step of the decoder, user direct connection to the encoder to focus on a particular part of the source sequence</p>
<p>실제로 사람이 번역을 할 때에도, source sentence를 읽고 곧바로 target sentence를 써내려가기보다는 target sentence를 작성하면서 source sentence를 다시 읽어보기도 하고, 계속 시선이 왔다갔다 한다. 이러한 컨셉을 direct connection으로 구현한 것이 Attention이다.</p>
<figure class="rehype-figure"><img src="/static/attention-bfbaee7f.gif" alt=""></figure>
<h4>Attention in equations</h4>
<p>CS224n Assignment 4 Handout : Attention with Bidirectional LSTMs</p>
<div class="rehype-figure-container"><figure class="rehype-figure"><img src="/static/19-2bf525c2.PNG" alt=""></figure><figure class="rehype-figure"><img src="/static/20-907a5e93.PNG" alt=""></figure></div>
<h4>Attention is great!</h4>
<ul>
<li>Attention significantly improves NMP performance</li>
<li>Attention solves the bottleneck problem</li>
<li>Attention helps with vanishing gradient problem</li>
<li>Attention provides some interpretability
<ul>
<li>By inspecting attention distribution, we can get (soft) <strong>alignment for free!</strong></li>
<li>The network just learned alignment by itself</li>
</ul>
</li>
</ul>
<h4>Attention is a general Deep Learning technique</h4>
<ul>
<li>You can use attention in "many architectures" (not just seq2seq) and "many tasks" (not just MT)</li>
<li>More general definition of Attention</li>
</ul>
<blockquote>
<p>Given a set of vector <strong>values</strong>, and a vector <strong>query</strong>, attention is a technique to compute a weighted sum of the values, dependent on the query</p>
</blockquote>
<ul>
<li>We somtimes say that the "query attends to the values."
<ul>
<li>e.g. in the seq2seq + attention model, each <strong>decoder hidden state (query)</strong> attends to all the <strong>encoder hiddent states (values)</strong>.</li>
</ul>
</li>
</ul>
<p><a href="https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms">StackExchange : What exactly are keys, queries, and values in attention mechanisms?</a></p>
<p>Attention에서 말하는 key, query, 그리고 value가 무엇인지 선뜻 이해되기 어렵다. 위 StackExchange 답변에 자세히 설명되어있으며, 이해한 바를 옮기자면 다음과 같다.</p>
<p>일단, key, query, value는 Retrieval System에서 통용되는 개념이다. 가령 유튜브에서 영상을 검색한다고 하면, 각각 아래와 같은 의미를 가진다.</p>
<blockquote>
<p>The key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best matched videos (values).</p>
</blockquote>
<p>그리고 Attention model에서 어떤 값이 각각 key, query, value에 대응되는지 알아야 그 다음을 이해할 수 있다.</p>
<figure class="rehype-figure"><img src="/static/21-75a569b4.PNG" alt=""></figure>
<ul>
<li>key는 첫번째 식에서의 \(h_1,\cdots ,h_N\)</li>
<li>query는 첫번째 식에서의 \(s_t\)</li>
<li>value는 세번째 식에서의 \(h_i\)에 대응된다.</li>
</ul>
<p>우선, Attention이란 무엇인가? 바로 각 step마다 source sentence 중 어떤 부분에 "attention"을 두고  단어를 생성(if MT)할지 결정하는 것이다. \(\alpha^t\)를 가중치로 value들의 weighted sum을 계산한다는 말은, 큰 가중치를 갖는 부분에 큰 "attention"을 둔다는 것을 의미한다.</p>
<p>\(\alpha^t\)를 softmax가 아닌 one-hot vector라고 생각하면 더욱 명확해진다. one-hot vector라면 encoder가 생성한 수개의 hidden state (value) 중 단 하나만 골라서 이 부분에'만' attention을 두고 decoding을 하게되는 것이다. 더 나아가, one-hot vector의 마지막 원소가 1이라면 이는 Simple seq2seq과 동치임을 알 수 있다.</p>
<p>그렇다면 다시 key, query, value의 개념으로 돌아가서, 왜 key와 query의 내적값으로 가중치 \(\alpha^t\)를 구하는걸까? 위의 유튜브 검색 비유를 통해 보자면, key를 query에 mapping하여 그 값을 토대로 value를 산출한다. Attention에서는 key와 value를 mapping하는 방법으로 벡터간의 내적을 채택하는 것이다.</p>
<p>Decoder의 매 Step마다 query (\(s_t\))는 매번 바뀌고, key (\(h_1,\cdots ,h_N\))는 고정된 값임을 되짚어본다면 왜 key와 query인지 이해할 수 있을 것이다.</p>
<h4>Attention variants</h4>
<p>\(\alpha^t = softmax(e^t)\) 에서  \(e^t\) 를 계산하는 방법으로 dot-product만 논했으나, 여러가지 방법이 더 있다.</p>
<ul>
<li>Basic dott-product attention:
\[e_i=s^Th_i\in \mathbb{R} \]</li>
<li>Multiplicative attention
\[e_i=s^TWh_i\in \mathbb{R} \]</li>
<li>Reduced rank multiplicative attention
\[e_i=s^T(U^TV)h_i=(Us)^T(Vh_i)\in \mathbb{R} \]</li>
<li>Additive attention
\[e_i=v^Ttanh(W_1h_i+W_2s) \in \mathbb{R} \]</li>
</ul></div></article><!--$--><!--/$--></main><footer class="border-t mt-12 py-6"><div class="container mx-auto px-4 text-center text-gray-500"><p>© <!-- -->2025<!-- --> jinh0park. All Rights Reserved.</p></div></footer></div><script src="/_next/static/chunks/webpack-b66e35ffe30362a3.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[2619,[\"619\",\"static/chunks/619-ba102abea3e3d0e4.js\",\"953\",\"static/chunks/app/blog/%5Bslug%5D/page-7cf65f3911bce8b7.js\"],\"\"]\n3:I[9766,[],\"\"]\n4:I[8924,[],\"\"]\n6:I[4431,[],\"OutletBoundary\"]\n8:I[5278,[],\"AsyncMetadataOutlet\"]\na:I[4431,[],\"ViewportBoundary\"]\nc:I[4431,[],\"MetadataBoundary\"]\nd:\"$Sreact.suspense\"\nf:I[7150,[],\"\"]\n:HL[\"/_next/static/media/4cf2300e9c8272f7-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/media/93f479601ee12b01-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n:HL[\"/_next/static/css/8cd908196c5f99fa.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"zp11YICo0no68g9G9pVjw\",\"p\":\"\",\"c\":[\"\",\"blog\",\"cs224n-summary\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"cs224n-summary\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/8cd908196c5f99fa.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"ko\",\"children\":[\"$\",\"body\",null,{\"className\":\"__variable_188709 __variable_9a8899 antialiased\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex flex-col min-h-screen\",\"children\":[[\"$\",\"header\",null,{\"className\":\"bg-gray-100 dark:bg-gray-900 border-b\",\"children\":[\"$\",\"div\",null,{\"className\":\"container mx-auto px-4 py-4 flex justify-between items-center\",\"children\":[[\"$\",\"$L2\",null,{\"href\":\"/\",\"className\":\"text-xl font-bold hover:text-blue-600\",\"children\":\"jinh0park = jin + h0 + park\"}],[\"$\",\"nav\",null,{\"children\":[\"$\",\"a\",null,{\"href\":\"https://github.com/jinh0park\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-gray-600 dark:text-gray-300 hover:text-black dark:hover:text-white\",\"children\":\"GitHub\"}]}]]}]}],[\"$\",\"main\",null,{\"className\":\"flex-grow container mx-auto px-4 py-8\",\"children\":[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"footer\",null,{\"className\":\"border-t mt-12 py-6\",\"children\":[\"$\",\"div\",null,{\"className\":\"container mx-auto px-4 text-center text-gray-500\",\"children\":[\"$\",\"p\",null,{\"children\":[\"© \",2025,\" jinh0park. All Rights Reserved.\"]}]}]}]]}]}]}]]}],{\"children\":[\"blog\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[[\"slug\",\"cs224n-summary\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L3\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L4\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L5\",null,[\"$\",\"$L6\",null,{\"children\":[\"$L7\",[\"$\",\"$L8\",null,{\"promise\":\"$@9\"}]]}]]}],{},null,false]},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[[\"$\",\"$La\",null,{\"children\":\"$Lb\"}],[\"$\",\"meta\",null,{\"name\":\"next-size-adjust\",\"content\":\"\"}]],[\"$\",\"$Lc\",null,{\"children\":[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$d\",null,{\"fallback\":null,\"children\":\"$Le\"}]}]}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$f\",[]],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"10:T865a,"])</script><script>self.__next_f.push([1,"\u003ch1\u003eLecture 1 : Introduction and Word Vectors ~ Lecture 2 : Word Vectors 2 and Word Window Classification\u003c/h1\u003e\n\u003ch2\u003eI. Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eHuman language and word meaning\n\u003cul\u003e\n\u003cli\u003eHow do we represent the meaning of a word?\u003c/li\u003e\n\u003cli\u003eWord2Vec\u003c/li\u003e\n\u003cli\u003eGloVe\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSummary\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eII. Human language and word meaning\u003c/h2\u003e\n\u003ch3\u003e1. How do we represent the meaning of a word?\u003c/h3\u003e\n\u003ch4\u003e(1) Definition : \u003cstrong\u003emeaning\u003c/strong\u003e\u003c/h4\u003e\n\u003cp\u003eWebster 영영사전 : the idea that is represented by a word, phrase, etc.\u003c/p\u003e\n\u003cp\u003e표준국어대사전(\"의미\") : 말이나 글의 뜻\u003c/p\u003e\n\u003ch4\u003e(2) Commonest linguistic way of thinking of meaning :\u003c/h4\u003e\n\u003cp\u003e\\[signifier(symbol) ↔ signified(idea or thing)\\]\u003c/p\u003e\n\u003ch4\u003e(3) Common NLP solution\u003c/h4\u003e\n\u003cp\u003e\u003cem\u003eWordNet\u003c/em\u003e, a thesaurus containing lists of synonym sets and hypernyms\u003c/p\u003e\n\u003cp\u003e동의어, 유의어(synonyms) 또는 상의어(hypernyms)로 이루어진 사전을 통해 의미를 정의할 수 있다.\u003c/p\u003e\n\u003cp\u003e※ WordNet의 문제점\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e뉘앙스를 담을 수 없다.\u003c/li\u003e\n\u003cli\u003e단어의 새로운 의미를 담을 수 없다.\u003c/li\u003e\n\u003cli\u003e동의어 등을 판단하는 기준이 주관적이다.\u003c/li\u003e\n\u003cli\u003e제작하는데 인력이 많이 소모된다.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWord Similarity를 계산할 수 없다.\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e(4) Representing words as discrete symbols\u003c/h4\u003e\n\u003cp\u003e\"One-hot vector\" representing :\n\\[motel = [0, 0, 0, 0, 0, 1, 0] \\ hotel = [0, 0, 0, 1, 0, 0, 0]\\]\u003c/p\u003e\n\u003cp\u003e※ One-hot vector representing의 문제점\n모든 vector들이 orthogonal하기 때문에, similarity를 계산할 수 없다.\u003c/p\u003e\n\u003ch4\u003e(5) Representing words by their context\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eDistributional semantics\u003c/strong\u003e : A word's meaning is given by the words that frequency appear close-by.\nWhen a word \\(w\\) appears in a textm its context is the set of words that appears nearby.\u003c/p\u003e\n\u003cp\u003e특정 단어의 의미는, 글에서 그 특정 단어 주변에 어떤 단어들(=context)이 주로 오는지에 따라 파악할 수 있다.\n위 아이디어를 이용하여, 단어를 Vectorize 할 수 있다.\u003c/p\u003e\n\u003cp\u003eNote : Word vectors are also called word embeddings or word representations, They are a \u003cstrong\u003edistributed representation\u003c/strong\u003e.\u003c/p\u003e\n\u003ch4\u003e(6) Conclusion\u003c/h4\u003e\n\u003cp\u003eNLP에서 단어를 표현하는 방식으로 WordNet, discrete representations, distributed representation 등이 있다.\n이때 NLP에서 \"유용한\" 방식으로 단어를 표현하기 위해서는, 단어 간 유사도(Similarity)를 계산할 수 있도록 distributed representation이 가장 적절한 해법이다.\nDistributional semantics에서 착안하여, 단어들을 실수 벡터로 표현하는 방법으로 Word2Vec을 알아본다.\u003c/p\u003e\n\u003ch3\u003e2. Word2Vec\u003c/h3\u003e\n\u003ch4\u003e(1) What is Word2Vec?\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eWord2Vec\u003c/strong\u003e (Mikolov et al. 2013, \u003ca href=\"http://arxiv.org/pdf/1301.3781.pdf\"\u003ePDF\u003c/a\u003e) is a framework for learning word vectors.\u003c/p\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/1-6eb1b2d1.PNG\" alt=\"\"\u003e\u003c/figure\u003e\n\u003cp\u003e\\[Likelihood = L(\\theta) = \\prod_{t=1}^{T} \\prod_{-m \\le j\\le m, j\\ne0} P(w_{t+j}|w_t;\\theta)\\]\n\\[objective function = J(\\theta) = -\\frac{1}{T}log(L(\\theta))\\]\u003c/p\u003e\n\u003cp\u003e특정 단어에 대하여, 주변(Window)에 다른 단어들이 나타날 확률의 곱 (첫번째 \\(\\prod\\))을\n모든 단어마다 계산해 곱해주면 (두번째 \\(\\prod\\)), 이를 Likelihood라고 하며,\n-Log를 취하여 objective function을 최소화 함으로써 word representation을 찾을 수 있다.\u003c/p\u003e\n\u003ch5\u003eLikelihood란?\u003c/h5\u003e\n\u003cp\u003e관측 데이터 \\(x\\)가 있을 때, 어떤 분포 \\(\\theta\\)를 주고, 그 분포에서 데이터가 나왔울 확률을 말한다.\n\\[L(\\theta|x)=P_{\\theta}(X=x)=\\prod_{k=1}^n P(x_k|\\theta)\\]\nWord2Vec에서 \\(\\theta\\)란 \"to be defined in terms of our vectors\", 즉 optimze할 word vector들의 값을 의미하며(이에 따라 분포가 정해진다),\n위 Likelihood의 정의로부터 본다면, \\(w_t\\)마다 likelihood를 구한 후 T번 곱하는 것이라고 볼 수 있겠다.\u003c/p\u003e\n\u003ch4\u003e(2) How to calculate the probability?\u003c/h4\u003e\n\u003cp\u003e\\[P(o|c) = \\frac{exp(u_o^Tv_c)}{\\sum_{w\\in V}exp(u_w^Tv_c)}\\]\nThis is an example of the \u003cstrong\u003esoftmax function\u003c/strong\u003e.\nThe softmax fuction maps arbitrary values \\(x_i\\) to a probability distribution \\(p_i\\)\n(\"max\" because amplifies probability to smaller \\(x_i\\), \"soft\" because still assigns some probability to smaller \\(x_i\\))\u003c/p\u003e\n\u003ch4\u003e(3) Optimization\u003c/h4\u003e\n\u003cp\u003eGradient descent, chain rule, SGD ... (생략)\u003c/p\u003e\n\u003ch4\u003e(4) Word2Vec : More details\u003c/h4\u003e\n\u003col\u003e\n\u003cli\u003eTwo model variants\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eSkip-grams (SG) ☞ 위에서 한 내용\u003c/li\u003e\n\u003cli\u003eContinuous Bag of Words (CBOW)\u003c/li\u003e\n\u003c/ul\u003e\n\u003col start=\"2\"\u003e\n\u003cli\u003eAdditional efficiency in Training\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003eNaive softmax\u003c/li\u003e\n\u003cli\u003eNegative samlping\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e(5) The skip-gram model with negative sampling \u003ca href=\"http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf\"\u003ePDF\u003c/a\u003e\u003c/h4\u003e\n\u003cp\u003eMain idea : train binary logistic regressions for a true pair(center word and a word in its context window) versus several noise pairs (the center word paired with a random word)\u003c/p\u003e\n\u003cp\u003eLikelihood를 계산하는 식 중, softmax는 computationally expensive하므로(vocabulary에 있는 모든 단어에 대해 내적 필요), 이를 Negative-sampling을 이용한 방식으로 대체한다.\u003c/p\u003e\n\u003ch3\u003e3. GloVe\u003c/h3\u003e\n\u003ch4\u003e(1) Co-occurrence matrix K\u003c/h4\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/2-e6e5c360.PNG\" alt=\"\"\u003e\u003c/figure\u003e\n\u003cul\u003e\n\u003cli\u003eWindow length 1 (most common : 5-10)\u003c/li\u003e\n\u003cli\u003e[\"I like deep learning\", \"I like NLP\", \"I enjoy flying\"]\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCo-occurrence matrix는 V * V 크기의 행렬이고, sparsity issue가 있으므로 차원을 낮추는 과정이 필요하며, 대표적으로 SVD(Singular Value Decomposition)이 있다.\u003c/p\u003e\n\u003cp\u003e이때, raw counts에 SVD를 바로 적용하는 것은 잘 작동하지 않고, the, he, has와 같이 과도하게 자주 등장하는 단어들(function words)의 count를 clipping하거나, count에 log를 취하거나, function words를 무시하는 방법 등을 추가로 요한다.\u003c/p\u003e\n\u003ch4\u003e(2) GloVe\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"http://nlp.stanford.edu/pubs/glove.pdf\"\u003eGloVe: Global Vectors for Word Representation\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eQ: How to we capture ratios of co-occurrence probabilities as linear meaning components in a word vector space?\u003c/p\u003e\n\u003cp\u003eA: Log-bilinear model (아래는 with vector differences)\n\\[w_i\\cdot w_j=logP(i|j) \\ w_x\\cdot (w_a-w_b) = log {\\frac{P(x|a)}{P(x|b)}}\\]\u003c/p\u003e\n\u003cp\u003e\\(w_i\\cdot w_j=logP(i|j)\\) 가 되도록 \\(w\\)를 optimize하는 것이 목표이다. \\(P\\)는 co-occurrence matrix로 부터 계산됨.\u003c/p\u003e\n\u003cp\u003e\\[J = \\sum^V_{i,j=1}f(X_{ij})(w_i^T\\tilde w_j+b_i+\\tilde b_j - logX_{ij})^2\\]\u003c/p\u003e\n\u003cp\u003e\\(f\\) -\u003e clipping function fot function words\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eFast training\u003c/li\u003e\n\u003cli\u003eScalable to huge corpora\u003c/li\u003e\n\u003cli\u003eGood performance even with small corpus and small vectors\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e(3) Evaluation of word vectors\u003c/h4\u003e\n\u003cp\u003eRelated to general evaluation in NLP : Intrinsic vs. extrinsic\u003c/p\u003e\n\u003ch5\u003eIntrinsic word vector evaluation\u003c/h5\u003e\n\u003col\u003e\n\u003cli\u003eWord vector analogies (ex. man : woman = king : ???, ??? = queen)\u003c/li\u003e\n\u003cli\u003eWord vector distances and their correlation with human judgements\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch5\u003eExtrinsic word vector evaluation\u003c/h5\u003e\n\u003col\u003e\n\u003cli\u003eNamed entity recognition\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch4\u003e(4) Word senses and word sense ambiguity\u003c/h4\u003e\n\u003cp\u003eMost words have lots of meanings!\u003c/p\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/3-6a47b122.PNG\" alt=\"\"\u003e\u003c/figure\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"http://www.aclweb.org/anthology/Q15-1016\"\u003eImproving Distributional Similarity with Lessons Learned from Word Embeddings\u003c/a\u003e Huang et al. 2012\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://transacl.org/ojs/index.php/tacl/article/viewFile/1346/320\"\u003eLinear Algebraic Structure of Word Senses, with Applications to Polysemy\u003c/a\u003e Arora et al. 2018\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1\u003eIII. Summary\u003c/h1\u003e\n\u003cp\u003eHuman language를 컴퓨터가 이해할 수 있도록 표현하기 위해서는 \"word vector\"를 설계해야한다. 이를 위한 시도들로 Word2Vec, GloVe를 알아보았다. Word2Vec은 SG 또는 CBOW로 구현할 수 있고, gradient descent를 효율적으로 하기 위해 negative sampling을 도입할 수 있다. GloVe는 co-occurrence matrix를 이용하여 word vector를 구한다.\u003c/p\u003e\n\u003cp\u003eWord vector를 만들었으면 이를 평가할 수 있어야 한다. NLP에서 평가 지표로 Intrinsic evaluation 또는 Extrinsic evaluation이 있다.\u003c/p\u003e\n\u003chr\u003e\n\u003ch1\u003eLecture 3 : Backprop and Neural Networks ~ Lecture 4 : Dependency Parsing\u003c/h1\u003e\n\u003ch2\u003eI. Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eBackProp and Neural Networks\n\u003cul\u003e\n\u003cli\u003eSimple NER\u003c/li\u003e\n\u003cli\u003eBackpropagation\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDependency Parsing\n\u003cul\u003e\n\u003cli\u003eSyntactic Structure : Constituency and Dependency\u003c/li\u003e\n\u003cli\u003eDependecy Grammar and Treebanks\u003c/li\u003e\n\u003cli\u003eTransition-based dependency parsing\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSummary\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eII. BackProp and Neural Networks\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\n\u003cp\u003eSimple NER : Window classification using binary logistic classifier\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eBackpropagation\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eCS231n에서 배웠던 내용과 대부분 중복되므로, 이해하는데 유용한 슬라이드 몇 장만 아카이브.\u003c/p\u003e\n\u003cdiv class=\"rehype-figure-container\"\u003e\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/cs224n-2021-lecture03-neuralnets_38-b86ba0bd.jpg\" alt=\"\"\u003e\u003c/figure\u003e\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/cs224n-2021-lecture03-neuralnets_39-08e9b207.jpg\" alt=\"\"\u003e\u003c/figure\u003e\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/cs224n-2021-lecture03-neuralnets_40-c2e59963.jpg\" alt=\"\"\u003e\u003c/figure\u003e\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/cs224n-2021-lecture03-neuralnets_41-d8fbe093.jpg\" alt=\"\"\u003e\u003c/figure\u003e\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/cs224n-2021-lecture03-neuralnets_54-f2a1f476.jpg\" alt=\"\"\u003e\u003c/figure\u003e\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/cs224n-2021-lecture03-neuralnets_65-10582ea5.jpg\" alt=\"\"\u003e\u003c/figure\u003e\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/cs224n-2021-lecture03-neuralnets_72-f128ba3f.jpg\" alt=\"\"\u003e\u003c/figure\u003e\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/cs224n-2021-lecture03-neuralnets_73-7079b025.jpg\" alt=\"\"\u003e\u003c/figure\u003e\u003c/div\u003e\n\u003ch2\u003eIII. Dependency Parsing\u003c/h2\u003e\n\u003ch3\u003e1. Syntactic Structure : Constituency and Dependency\u003c/h3\u003e\n\u003ch4\u003e(1) Constituency\u003c/h4\u003e\n\u003cp\u003ePhrase structure organizes words into nested constituents.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eStarting unit : words\n\u003cul\u003e\n\u003cli\u003ethe, cat, cuddly, by, door\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eWords combine into phrases\n\u003cul\u003e\n\u003cli\u003ethe cuddly cat, by the door\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003ePhrases can combine into bigger phrases\n\u003cul\u003e\n\u003cli\u003ethe cuddly cat by the door\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003e(2) Dependency\u003c/h4\u003e\n\u003cp\u003eDependency structure shows which words depend on (modify, attach to, or are arguments of) which other words.\u003c/p\u003e\n\u003ch4\u003e(3) Sentence structure\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eWhy do we need sentence structure?\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003eHumans communicate complex ideas by composing words together into bigger units to convey complex meanings. Listeners need to work out what modifies what. A model needs to understand sentence structure in order to be able to interpret language correctly.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eAmbiguities\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePrepositional phrase attachment ambiguity\u003c/li\u003e\n\u003cli\u003eCoordination scope ambiguity\u003c/li\u003e\n\u003cli\u003eAdjectival/Advebial modifier ambiguity\u003c/li\u003e\n\u003cli\u003eVerb phrase attachment ambiguity\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e2. Dependecy Grammar and Treebanks\u003c/h3\u003e\n\u003ch4\u003e(1) Dependecy Grammar\u003c/h4\u003e\n\u003cp\u003eDependency syntax postulates that syntactic structure consists of relations between lexical itemns, normally binary asymmetric relations (\"arrows\") called \u003cstrong\u003edependencies\u003c/strong\u003e\u003c/p\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/4-fcdc4d4d.PNG\" alt=\"\"\u003e\u003c/figure\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/5-b67cd3de.PNG\" alt=\"\"\u003e\u003c/figure\u003e\n\u003cp\u003eWe usually add a fake ROOT so every word is a dependent of precisely 1 other node.\u003c/p\u003e\n\u003ch4\u003e(2) Treebanks\u003c/h4\u003e\n\u003cp\u003eThe rise of annotated data \u0026#x26; Universal dependencies treebanks\u003c/p\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/6-1f8aa17e.PNG\" alt=\"\"\u003e\u003c/figure\u003e\n\u003cp\u003eA treebank gives us many things\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eReusability of the labor\u003c/li\u003e\n\u003cli\u003eBroad coverage, not just a few intuitions\u003c/li\u003e\n\u003cli\u003eFrequencies and distributional information\u003c/li\u003e\n\u003cli\u003eA way to evaluate NLP systems\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eSources of information for dependency parsing\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBilexical affinities\u003c/li\u003e\n\u003cli\u003eDependecy distances\u003c/li\u003e\n\u003cli\u003eIntervening material\u003c/li\u003e\n\u003cli\u003eValency of heads\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e3. Transition-based dependency parsing\u003c/h3\u003e\n\u003ch4\u003eBasic transition-based dependency parser\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eArc-standard transition-based parser\n\u003cul\u003e\n\u003cli\u003e3 actions : SHIFT, LECT-ARC, RIGHT-ARC\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eMaltParser\n\u003cul\u003e\n\u003cli\u003eEach action is predicted by a discriminative classfier over each legal move.\u003c/li\u003e\n\u003cli\u003eIt provides very fast linear time parsing, with high accuracy - great for parsing the web.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e4. Neural dependency parsing\u003c/h3\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/7-d0f6ad0c.PNG\" alt=\"\"\u003e\u003c/figure\u003e\n\u003cp\u003eNeural networds can accurately determine the structure of sentences, supporting interpretation.\u003c/p\u003e\n\u003ch2\u003eIV. Summary\u003c/h2\u003e\n\u003cp\u003e문장을 이해하기 위해서는 각 단어가 어떤 단어를 수식하고 있는지 그 관계를 파악해야한다. 이를 Dependency라고 하며, 주어진 문장에 대해 그 수식 관계를 파악하는 것을 Dependency parsing이라고 한다. 수많은 corpus에 대해 Dependency parsing을 하는 것은 NLP에서 중요한 과제 중 하나이다.\u003c/p\u003e\n\u003cp\u003eTransition-based dependency parsing은 Stack, Buffer 개념을 도입하여 알고리즘으로 Dependency parsing을 하는 기법이다. MaltParser은 머신러닝을 이용해 높은 성능을 보여주었으며, 비교적 최근 인공신경망을 이용한 Neural dependency parsing은 가장 강력한 기법 중 하나로 자리잡았다. 그 외에도 Graph-based dependency parser 등이 있다.\u003c/p\u003e\n\u003chr\u003e\n\u003ch1\u003eLecture 5 : Recurrent Neural Networks and Language Models ~ Lecture 6 : Vanishing Gradients, Fancy RNNs, Seq2Seq\u003c/h1\u003e\n\u003ch2\u003eI. Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eNeural Dependency Parsing\n\u003cul\u003e\n\u003cli\u003eNeural Dependency Parsing\u003c/li\u003e\n\u003cli\u003eA bit more about neural Networks\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eLanguage Modeling and RNNs\n\u003cul\u003e\n\u003cli\u003eLanguage Modeling\u003c/li\u003e\n\u003cli\u003eN-grams Language Models\u003c/li\u003e\n\u003cli\u003eNeural Language Models\u003c/li\u003e\n\u003cli\u003eEvaluating Language Models\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eLSTM : Long Short-Term Memory RNNs\n\u003cul\u003e\n\u003cli\u003eProblems with Vanishing and Exploding Gradients\u003c/li\u003e\n\u003cli\u003eLSTMs\u003c/li\u003e\n\u003cli\u003eMore about vanishing/exploding gradient Problem\u003c/li\u003e\n\u003cli\u003eBidirectional and Multi-layer RNNs: Motivation\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eSummary\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eII. Neural Dependency Parsing\u003c/h2\u003e\n\u003ch3\u003e1. Neural Dependency Parsing\u003c/h3\u003e\n\u003cp\u003eDeep learning classifiers are non-linear classifiers (cf. Traditional ML classifiers only give linear decision boundaries)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.emnlp2014.org/papers/pdf/EMNLP2014082.pdf\"\u003eA Fast and Accurate Dependency Parser using Neural Networks\u003c/a\u003e Chen and Manning, 2014\u003c/p\u003e\n\u003ch3\u003e2. A bit more about neural networks\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eRegularization\u003c/li\u003e\n\u003cli\u003eDropout\u003c/li\u003e\n\u003cli\u003eVectorization\u003c/li\u003e\n\u003cli\u003eNon-linearities\u003c/li\u003e\n\u003cli\u003eParameter initialization\u003c/li\u003e\n\u003cli\u003eOptimizers\u003c/li\u003e\n\u003cli\u003eLearning rates\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eIII. Language modeling and RNNs\u003c/h2\u003e\n\u003ch3\u003e1. Language Modeling\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eLanguage Modeling\u003c/strong\u003e is the task of predicting what word comes next.\u003c/p\u003e\n\u003cp\u003eMore formally: given a sequence of words \\(x^{(1)},x^{(2)},\\cdots,x^{(t)}\\), compute the probability distribution of the next word \\(x^{(t+1)}\\)\u003c/p\u003e\n\u003cp\u003e\\[P(x^{(t+1)}\\mid x^{(t)},\\cdots x^{(1)})\\]\u003c/p\u003e\n\u003cp\u003ewhere \\(x^{(t+1)}\\) can be any word in the vocabulary \\(V = {w_1, \\cdots , w_{\\mid V\\mid }}\\)\u003c/p\u003e\n\u003cp\u003e주어진 단어들의 sequence가 있을 때, 그 다음에 올 단어의 확률분포를 구하는 것을 Language modeling이라고 한다.\u003c/p\u003e\n\u003cp\u003e각 sequence step마다 \"단어가 다음에 올 확률\"을 곱하면 전체 텍스트의 확률 분포가 되며, 식은 아래와 같다.\u003c/p\u003e\n\u003cp\u003e\\[P(x^{(1)},\\cdots ,x^{(T)})=P(x^{(1)})\\times P(x^{(2)\\mid x^(1)}) \\times \\cdots \\ = \\prod_{t=1}^T P(x^{(t)}\\mid x^{(t-1)},\\cdots ,x^{(1)}) \\]\u003c/p\u003e\n\u003ch3\u003e2. N-gram Language Models\u003c/h3\u003e\n\u003cp\u003eIdea : Collect statistics about how frequent different n-grams are and use these to predict next word.\u003c/p\u003e\n\u003cp\u003eFirst we make a \u003cstrong\u003eMarkov assumption\u003c/strong\u003e : \\(x^{(t+1)}\\) depends only on the preceding \\(n-1\\) words\u003c/p\u003e\n\u003cp\u003e\\[P(x^{(t+1)}\\mid x^{(t)},\\cdots x^{(1)}) = P(x^{(t+1)}\\mid x^{(t)},\\cdots x^{(t-n+2)})\n\\ = \\frac {P(x^{(t+1)},x^{(t)},\\cdots x^{(t-n+2)})} {P(x^{(t)},\\cdots x^{(t-n+2)})} \\]\u003c/p\u003e\n\u003cp\u003e\\[\\approx \\frac {count(x^{(t+1)},x^{(t)},\\cdots x^{(t-n+2)})} {count(x^{(t)},\\cdots x^{(t-n+2)})}\\]\u003c/p\u003e\n\u003ch4\u003eProblems of N-gram\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eSparsity Problems\n\u003cul\u003e\n\u003cli\u003eProblem 1 : 위 식에서 분자 부분의 count가 0이라면, 해당 단어의 확률이 0으로 고정됨 \u003cbr\u003e\nSolution : Add small \\(\\delta\\) to the count for every \\(w \\in V\\)\u003c/li\u003e\n\u003cli\u003eProblem 1 : 위 식에서 분모 부분의 count가 0이라면, 그 다음 단어의 확률을 정의할 수 없음 \u003cbr\u003e\nSolution : 마지막 단어 하나 생략하고 찾기\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eStorage Problems\n\u003cul\u003e\n\u003cli\u003eNeed to store count for all n-grams you saw in the corpus.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eResults : Surprisingly grammatical!\u003c/p\u003e\n\u003cp\u003e...but \u003cstrong\u003eincoherent\u003c/strong\u003e. We need to consider more than three words at a time if we want to model language well. But increasing n worsens \u003cstrong\u003esparsity problem\u003c/strong\u003e, and increase model size.\u003c/p\u003e\n\u003ch3\u003e3. Neural Language Models\u003c/h3\u003e\n\u003ch4\u003e(1) A fixed-window neural Language Model\u003c/h4\u003e\n\u003cp\u003e\u003ca href=\"https://jmlr.org/papers/volume3/tmp/bengio03a.pdf\"\u003eA Neural Probabilistic Language Model\u003c/a\u003e, Y.Bengio, et al. (2000/2003)\u003c/p\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/8-69587cfc.png\" alt=\"\"\u003e\u003c/figure\u003e\n\u003ch5\u003eImprovements over n-gram LM\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eNo sparsity Problem\u003c/li\u003e\n\u003cli\u003eDon't need to store all observed n-grams\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003eRemaining Problems\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eFixed window is too small\u003c/li\u003e\n\u003cli\u003eEnlarging window enlarges \\(W\\) ☞ Window can never be large enough!\u003c/li\u003e\n\u003cli\u003eNo symmetry in how the inputs are processed\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e☞ We need a neural architecture that can process any length input!\u003c/p\u003e\n\u003ch4\u003e(2) Recurrent Neural Networks\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eCore idea\u003c/strong\u003e : Apply the same weights \\(W\\) repeatedly!\u003c/p\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/9-ce32deb1.png\" alt=\"\"\u003e\u003c/figure\u003e\n\u003ch5\u003eRNN Advantages\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eCan process \u003cstrong\u003eany length\u003c/strong\u003e input\u003c/li\u003e\n\u003cli\u003eComputation for step \\(t\\) can (in theory[\u003cem\u003edue to gradient vanishing problem, \"in theory\"\u003c/em\u003e]) use information from many steps back\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eModel size doesn't increase\u003c/strong\u003e for longer input context\u003c/li\u003e\n\u003cli\u003eSame weights applied on every timestep, so there is \u003cstrong\u003esymmetry\u003c/strong\u003e in how inputs are processed.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003eRNN Disadvantages\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eRecurrent computation is slow (it runs in the for loop, can't be computed parallelly)\u003c/li\u003e\n\u003cli\u003eIn practice, difficult to access information from many steps back\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003eTraining an RNN Language Models\u003c/h5\u003e\n\u003cp\u003e\\[J^{(t)}(\\theta)=CE(y^{(t)}, \\hat y^{(t)})=-\\sum_{w\\in V}y_w^{(t)}=-log\\hat y^{(t)}\u003cem\u003e{x\u003c/em\u003e{t+1}}\\]\u003c/p\u003e\n\u003cp\u003e\\[J(\\theta)=\\frac {1}{T} \\sum^T_{t=1}J^{(t)}(\\theta)\\]\u003c/p\u003e\n\u003ch3\u003e4. Evaluating Language Models\u003c/h3\u003e\n\u003cp\u003eThe standard evaluation metric for LM is \u003cstrong\u003eperplexity\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e\\[perplexity = \\prod_{t=1}^T (\\frac {1}{P_{LM}(x^{(t+1)}\\mid x^{(t)},\\cdots ,x^{(1)})})^{1/T}\n\\ = exp(J(\\theta))\\]\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLower\u003c/strong\u003e perplexity is better!\u003c/p\u003e\n\u003cp\u003eprobability of corpus의 기하평균의 역, 모든 단어를 정확히 맞춘다면 \\(perplexity = 1\\)\u003c/p\u003e\n\u003ch4\u003eWhy should we care about Language Modeling?\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eLanguage Modeling is a \u003cstrong\u003ebenchmark task\u003c/strong\u003e that helps us \u003cstrong\u003emeasure our progress\u003c/strong\u003e on understanding language\u003c/li\u003e\n\u003cli\u003eLanguage Modeling is a \u003cstrong\u003esubcomponent\u003c/strong\u003e of many NLP tasks\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eIV. LSTM : Long Short-Term Memory RNNs\u003c/h2\u003e\n\u003ch3\u003e1. Problems with Vanishing and Exploding Gradients\u003c/h3\u003e\n\u003ch4\u003eVanishing gradients\u003c/h4\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/10-e51da064.png\" alt=\"\"\u003e\u003c/figure\u003e\n\u003ch4\u003eExploding gradients\u003c/h4\u003e\n\u003cp\u003eExploding gradients can solve by simple methods such as \u003cstrong\u003egradient clipping\u003c/strong\u003e .\u003c/p\u003e\n\u003cp\u003eHow about a RNN with separate memory to fix the \u003cstrong\u003evanishing\u003c/strong\u003e gradient problem? ☞ \u003cstrong\u003eLSTMs\u003c/strong\u003e\u003c/p\u003e\n\u003ch3\u003e2. LSTMs\u003c/h3\u003e\n\u003cp\u003e\u003ca href=\"http://www.bioinf.jku.at/publications/older/2604.pdf\"\u003eHochreiter and Schmidhuber\u003c/a\u003e(1997)\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.jmlr.org/papers/volume3/gers02a/gers02a.pdf\"\u003eGers et al\u003c/a\u003e(2000) -\u003e Crucial part of the modern LSTM is here!\u003c/p\u003e\n\u003cp\u003eOn step \\(t\\), there is a hidden state \\(h^{(t)}\\) and a cell state \\(c^{(t)}\\)\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBoth are vectors length \\(n\\)\u003c/li\u003e\n\u003cli\u003eThe cell stores \u003cstrong\u003elong-term information\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe LSTM can read, erase, and write information from the cell\n\u003cul\u003e\n\u003cli\u003ethe cell becomes conceptually rather like RAM in a computer\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/11-f333b6f1.png\" alt=\"\"\u003e\u003c/figure\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/12-927dd4fb.png\" alt=\"\"\u003e\u003c/figure\u003e\n\u003cp\u003eLSTM이 Gradient vanishing을 해결할 수 있는 핵심 구조는 cell state이다. cell state는 곱연산이 아닌 \u003cstrong\u003e합연산\u003c/strong\u003e을 통해 다음 cell로 전해지므로, step을 오래 거치더라도 vanishing이 발생하지 않는다.\u003c/p\u003e\n\u003cp\u003ecell state에 long term memory가 저장되므로, hidden state를 계산할 때 output gate를 통해 long term memory에 저장된 정보를 얼마나 사용할지 결정할 수 있다.\u003c/p\u003e\n\u003ch4\u003e3. More about vanishing/exploding gradient Problem\u003c/h4\u003e\n\u003ch5\u003eIs vanishing/exploding gradient just a RNN problem?\u003c/h5\u003e\n\u003cp\u003eNo! It can be a problem for all neural architectures, especially very deep ones\u003c/p\u003e\n\u003cp\u003eSolution : Add more direct connections (e.g. ResNet, DenseNet, HighwayNet, and etc.)\u003c/p\u003e\n\u003ch4\u003e4. Bidirectional and Multi-layer RNNs: Motivation\u003c/h4\u003e\n\u003ch5\u003e(1) Bidirectional RNNs\u003c/h5\u003e\n\u003cp\u003e문장 구조상, 뒤에 있는 단어들 까지 보아야 단어의 의미를 파악할 수 있는 경우가 있다. (e.g. \u003cem\u003ethe movie was \"teriibly\" exciting!\u003c/em\u003e, \u003cem\u003eterribly\u003c/em\u003e가 긍정적인 의미로 사용되었음을 알기 위해서는 뒤의 \u003cem\u003eexciting\u003c/em\u003e도 보아야 한다.)\u003c/p\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/13-d1bf78b6.png\" alt=\"\"\u003e\u003c/figure\u003e\n\u003cp\u003e\\(h^{(t)}\\) has the dimension of \\(2d\\) (\\(d\\) is the hidden size of FW or BW)\u003c/p\u003e\n\u003cp\u003eNote: Bidirectional RNNs are only applicable if you have access to the \u003cstrong\u003eentire input sequence\u003c/strong\u003e ☞ Not applicable to LM!\u003c/p\u003e\n\u003ch5\u003e(2) Multi-layer RNNs\u003c/h5\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/14-c758b546.png\" alt=\"\"\u003e\u003c/figure\u003e\n\u003ch2\u003eV. Summary\u003c/h2\u003e\n\u003cp\u003eLanguage Modeling은 자연어처리에서 benchmark test \u0026#x26; subcomponent 역할을 하는 task이다. 딥러닝 이전에 N-grams LM이 존재하였으며, RNN을 도입하면서 성능이 비약적으로 상승하였다.\u003c/p\u003e\n\u003cp\u003e한편, RNN의 특성상 gradient vanishing(exploding) 문제가 발생하는데, 이를 해결하기 위해 RNNs중 하나로서 LSTMs이 도입되었다. LSTM의 핵심은 long term memory를 저장하는 cell state로, 합연산을 통해 값이 전달되므로 vanishing이 현저하게 줄어든다.\u003c/p\u003e\n\u003cp\u003eRNNs의 성능을 더욱 향상시키기 위한 시도로 Bidirectional, Multi-layer RNNs 등이 있으며, 오늘날 가장 좋은 성능을 보이는 모델 중 하나인 BERT와 같은 Transformer-based network에서도 이러한 구조들을 채택하고 있다.\u003c/p\u003e\n\u003chr\u003e\n\u003ch1\u003eLecture 7 : Machine Translation, Attention, Subword Models\u003c/h1\u003e\n\u003ch2\u003eI. Contents\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eMachine translation\u003c/li\u003e\n\u003cli\u003eSeq2seq\n\u003cul\u003e\n\u003cli\u003eNeural Machine Translation\u003c/li\u003e\n\u003cli\u003eTraining a Neural Machine Translation System\u003c/li\u003e\n\u003cli\u003eMulti-layer RNNs\u003c/li\u003e\n\u003cli\u003eDecoding varieties\u003c/li\u003e\n\u003cli\u003eEvaluating Maching Translation\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eAttention\n\u003cul\u003e\n\u003cli\u003eSeq2seq: the bottleneck problem\u003c/li\u003e\n\u003cli\u003eAttention\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eII. Machine Translation\u003c/h2\u003e\n\u003cp\u003eMachine Translation is the task of translating a sentence \\(x\\) from one language (\u003cstrong\u003ethe source language\u003c/strong\u003e) to a sentence \\(y\\) in another language (\u003cstrong\u003ethe target language\u003c/strong\u003e).\u003c/p\u003e\n\u003ch3\u003e1990s-2010s: Statistical Machine Translation\u003c/h3\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/15-28a58fc1.png\" alt=\"\"\u003e\u003c/figure\u003e\n\u003ch4\u003eAlignment\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eQ. How to learn translation model \\(P(x\\mid y)\\)?\n\u003cul\u003e\n\u003cli\u003eFirst, need large amount of parallel data e.g. \u003cem\u003eThe Rosetta Stone\u003c/em\u003e\u003c/li\u003e\n\u003cli\u003eBreak it down further: Introduce latent \\(a\\) variable into the model: \\(P(x, a\\mid y)\\) where \\(a\\) is the \u003cstrong\u003ealignment\u003c/strong\u003e, i.e. word-level correspondence between source sentence \\(x\\) and target sentence \\(y\\)\u003c/li\u003e\n\u003cli\u003eAlignments are \u003cstrong\u003elatent variables\u003c/strong\u003e: They aren't explicitly specified in the data!\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e언어마다 문법이나 단어 체계가 다르기 때문에, 번역을 하기 위해서는 source sentence와 target sentence 의 단어가 각각 어떻게 대응되는지 파악해야하며, 이를 alignment라고 한다.\u003c/p\u003e\n\u003cp\u003eAlignment는 one-to-one, many-to-one, one-to-many, many-to-many 등 복잡하게 구성되며, dependency parsing에서의 arc처럼 명시적으로 특정되지 않고 SMT에 내장되므로 \u003cstrong\u003elatent variable\u003c/strong\u003e이라고 부른다.\u003c/p\u003e\n\u003ch4\u003eDecoding for SMT\u003c/h4\u003e\n\u003cp\u003eEnumerating every possible \\(y\\) and calculate the probability is too expensive.\u003c/p\u003e\n\u003cp\u003eAnswer : Impose strong independence assumptions in model, use dynamic programming for globally optimal solutions\u003c/p\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/16-1a8ce14c.png\" alt=\"\"\u003e\u003c/figure\u003e\n\u003ch4\u003eConclusion\u003c/h4\u003e\n\u003cp\u003eThe best systems of SMT were \"extremely complex\"\u003c/p\u003e\n\u003ch2\u003eIII. Seq2Seq\u003c/h2\u003e\n\u003ch3\u003e1. Neural Machine translation\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eNeural Machine Translation (NMT)\u003c/strong\u003e is a way to do Machine Translation with a \u003cem\u003esingle end-to-end neural network\u003c/em\u003e\u003c/p\u003e\n\u003cp\u003eThe neural network architecture is called a \u003cstrong\u003esequence-to-sequence\u003c/strong\u003e model (a.k.a. \u003cstrong\u003eseq2seq\u003c/strong\u003e) and it involves \u003cstrong\u003etwo RNNs\u003c/strong\u003e\u003c/p\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/17-b8287b63.png\" alt=\"\"\u003e\u003c/figure\u003e\n\u003cul\u003e\n\u003cli\u003eseq2seq is useful for \u003cstrong\u003emore than just MT\u003c/strong\u003e\n\u003cul\u003e\n\u003cli\u003eSummarization\u003c/li\u003e\n\u003cli\u003eDialogue\u003c/li\u003e\n\u003cli\u003eParsing\u003c/li\u003e\n\u003cli\u003eCode generation\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eseq2seq model is an example of a \u003cstrong\u003eConditional Language Model\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e2. Training a Neural Machine Translation System\u003c/h3\u003e\n\u003cp\u003e\\[J(\\theta) = \\frac {1}{T} \\sum_{t=1}^T J_t\\]\n(\\(J_t\\) is negative lof prob of the word)\u003c/p\u003e\n\u003cp\u003eseqseq is optimized as a \u003cstrong\u003esingle system\u003c/strong\u003e, Backpropagation operates \"end-to-end\"\u003c/p\u003e\n\u003ch3\u003e3. Multi-layer RNNs\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003eHigh-performing RNNs are usually multi-layer : 2 to 4 layers!\u003c/li\u003e\n\u003cli\u003eUsually, skip-connections/dense-connections are needed to train deeper RNNs (e.g. 8 layers)\u003c/li\u003e\n\u003cli\u003eTransformer-based networks (e.g. BERT) are usually deeper, like 12 or 24 layers.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e4. Decoding varieties\u003c/h3\u003e\n\u003ch4\u003eGreedy Decoding\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eTake most probable word on each step\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eGreedy decoding has no way to undo decisions\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eExhaustive search Decoding\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eIdeally : We could tru computing all possible sequences \\(y\\) and find \\(y\\) that maximizes :\n\\[P(y\\mid x)=\\prod^T_{t=1}P(y_t\\mid y_1,\\cdots ,y_{t-1},x)\\]\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eThis \\(O(V^T)\\) complexity is far too expensive!!!\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eBeam search Decoding\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eCore idea : On each step of decoder, keep track of the \u003cem\u003ek most probable partial translations\u003c/em\u003e (which we call \u003cstrong\u003ehypotheses\u003c/strong\u003e)\u003c/li\u003e\n\u003cli\u003eBeam search is not guaranteed to find optimal solution, but much more efficient than exhaustive search.\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eAdvantages and Disadvantages of NMT\u003c/h4\u003e\n\u003ch5\u003eAdvantages\u003c/h5\u003e\n\u003cul\u003e\n\u003cli\u003eBetter performance\u003c/li\u003e\n\u003cli\u003eA single neural network to be optimized end-to-end\n\u003cul\u003e\n\u003cli\u003eNo subcomponents to be individually optimized\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eRequires much less human engineering effort\n\u003cul\u003e\n\u003cli\u003eNo feature engineering\u003c/li\u003e\n\u003cli\u003eSame method for all language pairs\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch5\u003eDisadvantages\u003c/h5\u003e\n\u003cp\u003eCompared to SMT :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLess interpretable\u003c/li\u003e\n\u003cli\u003eDifficult to control (e.g. can't easily specify rules or guidelines for translation)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3\u003e5. Evaluating Maching Translation\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003eBLEU\u003c/strong\u003e (Bilingual Evaluation Understudy)\u003c/p\u003e\n\u003cp\u003eBLEU compares the machine-written translation to one or several human-written translation(s), and computes a similarity score based on :\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003en-gram precision\u003c/li\u003e\n\u003cli\u003ePlus a penalty for too-short system translations\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"http://incredible.ai/nlp/2020/02/29/BLEU/\"\u003eLearn More - Incredible.AI : BLEU\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBLEU is useful, but imperfect\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2\u003eIV. Attention\u003c/h2\u003e\n\u003ch3\u003e1. Seq2seq: the bottleneck problem\u003c/h3\u003e\n\u003cp\u003eThe last hidden state of encoder which is fed to decoder \u003cstrong\u003eneeds to capture all information\u003c/strong\u003e about toe source sentence. ☞ \"Information Bottleneck!\"\u003c/p\u003e\n\u003ch3\u003e2. Attention\u003c/h3\u003e\n\u003ch4\u003eOverview\u003c/h4\u003e\n\u003cp\u003e\u003cstrong\u003eAttention\u003c/strong\u003e provides a solution to the bottleneck problem\u003c/p\u003e\n\u003cp\u003eCore idea: on each step of the decoder, user direct connection to the encoder to focus on a particular part of the source sequence\u003c/p\u003e\n\u003cp\u003e실제로 사람이 번역을 할 때에도, source sentence를 읽고 곧바로 target sentence를 써내려가기보다는 target sentence를 작성하면서 source sentence를 다시 읽어보기도 하고, 계속 시선이 왔다갔다 한다. 이러한 컨셉을 direct connection으로 구현한 것이 Attention이다.\u003c/p\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/attention-bfbaee7f.gif\" alt=\"\"\u003e\u003c/figure\u003e\n\u003ch4\u003eAttention in equations\u003c/h4\u003e\n\u003cp\u003eCS224n Assignment 4 Handout : Attention with Bidirectional LSTMs\u003c/p\u003e\n\u003cdiv class=\"rehype-figure-container\"\u003e\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/19-2bf525c2.PNG\" alt=\"\"\u003e\u003c/figure\u003e\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/20-907a5e93.PNG\" alt=\"\"\u003e\u003c/figure\u003e\u003c/div\u003e\n\u003ch4\u003eAttention is great!\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eAttention significantly improves NMP performance\u003c/li\u003e\n\u003cli\u003eAttention solves the bottleneck problem\u003c/li\u003e\n\u003cli\u003eAttention helps with vanishing gradient problem\u003c/li\u003e\n\u003cli\u003eAttention provides some interpretability\n\u003cul\u003e\n\u003cli\u003eBy inspecting attention distribution, we can get (soft) \u003cstrong\u003ealignment for free!\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eThe network just learned alignment by itself\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch4\u003eAttention is a general Deep Learning technique\u003c/h4\u003e\n\u003cul\u003e\n\u003cli\u003eYou can use attention in \"many architectures\" (not just seq2seq) and \"many tasks\" (not just MT)\u003c/li\u003e\n\u003cli\u003eMore general definition of Attention\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eGiven a set of vector \u003cstrong\u003evalues\u003c/strong\u003e, and a vector \u003cstrong\u003equery\u003c/strong\u003e, attention is a technique to compute a weighted sum of the values, dependent on the query\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003eWe somtimes say that the \"query attends to the values.\"\n\u003cul\u003e\n\u003cli\u003ee.g. in the seq2seq + attention model, each \u003cstrong\u003edecoder hidden state (query)\u003c/strong\u003e attends to all the \u003cstrong\u003eencoder hiddent states (values)\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms\"\u003eStackExchange : What exactly are keys, queries, and values in attention mechanisms?\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eAttention에서 말하는 key, query, 그리고 value가 무엇인지 선뜻 이해되기 어렵다. 위 StackExchange 답변에 자세히 설명되어있으며, 이해한 바를 옮기자면 다음과 같다.\u003c/p\u003e\n\u003cp\u003e일단, key, query, value는 Retrieval System에서 통용되는 개념이다. 가령 유튜브에서 영상을 검색한다고 하면, 각각 아래와 같은 의미를 가진다.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe key/value/query concept is analogous to retrieval systems. For example, when you search for videos on Youtube, the search engine will map your query (text in the search bar) against a set of keys (video title, description, etc.) associated with candidate videos in their database, then present you the best matched videos (values).\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e그리고 Attention model에서 어떤 값이 각각 key, query, value에 대응되는지 알아야 그 다음을 이해할 수 있다.\u003c/p\u003e\n\u003cfigure class=\"rehype-figure\"\u003e\u003cimg src=\"/static/21-75a569b4.PNG\" alt=\"\"\u003e\u003c/figure\u003e\n\u003cul\u003e\n\u003cli\u003ekey는 첫번째 식에서의 \\(h_1,\\cdots ,h_N\\)\u003c/li\u003e\n\u003cli\u003equery는 첫번째 식에서의 \\(s_t\\)\u003c/li\u003e\n\u003cli\u003evalue는 세번째 식에서의 \\(h_i\\)에 대응된다.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e우선, Attention이란 무엇인가? 바로 각 step마다 source sentence 중 어떤 부분에 \"attention\"을 두고  단어를 생성(if MT)할지 결정하는 것이다. \\(\\alpha^t\\)를 가중치로 value들의 weighted sum을 계산한다는 말은, 큰 가중치를 갖는 부분에 큰 \"attention\"을 둔다는 것을 의미한다.\u003c/p\u003e\n\u003cp\u003e\\(\\alpha^t\\)를 softmax가 아닌 one-hot vector라고 생각하면 더욱 명확해진다. one-hot vector라면 encoder가 생성한 수개의 hidden state (value) 중 단 하나만 골라서 이 부분에'만' attention을 두고 decoding을 하게되는 것이다. 더 나아가, one-hot vector의 마지막 원소가 1이라면 이는 Simple seq2seq과 동치임을 알 수 있다.\u003c/p\u003e\n\u003cp\u003e그렇다면 다시 key, query, value의 개념으로 돌아가서, 왜 key와 query의 내적값으로 가중치 \\(\\alpha^t\\)를 구하는걸까? 위의 유튜브 검색 비유를 통해 보자면, key를 query에 mapping하여 그 값을 토대로 value를 산출한다. Attention에서는 key와 value를 mapping하는 방법으로 벡터간의 내적을 채택하는 것이다.\u003c/p\u003e\n\u003cp\u003eDecoder의 매 Step마다 query (\\(s_t\\))는 매번 바뀌고, key (\\(h_1,\\cdots ,h_N\\))는 고정된 값임을 되짚어본다면 왜 key와 query인지 이해할 수 있을 것이다.\u003c/p\u003e\n\u003ch4\u003eAttention variants\u003c/h4\u003e\n\u003cp\u003e\\(\\alpha^t = softmax(e^t)\\) 에서  \\(e^t\\) 를 계산하는 방법으로 dot-product만 논했으나, 여러가지 방법이 더 있다.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBasic dott-product attention:\n\\[e_i=s^Th_i\\in \\mathbb{R} \\]\u003c/li\u003e\n\u003cli\u003eMultiplicative attention\n\\[e_i=s^TWh_i\\in \\mathbb{R} \\]\u003c/li\u003e\n\u003cli\u003eReduced rank multiplicative attention\n\\[e_i=s^T(U^TV)h_i=(Us)^T(Vh_i)\\in \\mathbb{R} \\]\u003c/li\u003e\n\u003cli\u003eAdditive attention\n\\[e_i=v^Ttanh(W_1h_i+W_2s) \\in \\mathbb{R} \\]\u003c/li\u003e\n\u003c/ul\u003e"])</script><script>self.__next_f.push([1,"5:[\"$\",\"article\",null,{\"className\":\"prose dark:prose-invert mx-auto py-8\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-4xl font-bold\",\"children\":\"강의 요약 - CS224n: Natural Language Processing with Deep Learning\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4 mt-2 mb-2\",\"children\":[[\"$\",\"p\",null,{\"className\":\"text-gray-500 !my-0\",\"children\":\"2022년 8월 27일\"}],[\"$\",\"$L2\",null,{\"href\":\"/categories/dev\",\"children\":[\"$\",\"span\",null,{\"className\":\"bg-gray-200 text-gray-800 text-sm font-medium px-3 py-1 rounded-full !my-0 hover:bg-gray-300\",\"children\":\"dev\"}]}]]}],[\"$\",\"hr\",null,{\"className\":\"!my-4\"}],[\"$\",\"div\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"$10\"}}]]}]\n"])</script><script>self.__next_f.push([1,"b:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n7:null\n"])</script><script>self.__next_f.push([1,"11:I[622,[],\"IconMark\"]\n"])</script><script>self.__next_f.push([1,"9:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"강의 요약 - CS224n: Natural Language Processing with Deep Learning | My Velite Blog\"}],[\"$\",\"meta\",\"1\",{\"property\":\"og:title\",\"content\":\"강의 요약 - CS224n: Natural Language Processing with Deep Learning\"}],[\"$\",\"meta\",\"2\",{\"property\":\"og:type\",\"content\":\"article\"}],[\"$\",\"meta\",\"3\",{\"property\":\"article:published_time\",\"content\":\"2022-08-27T00:00:00.000Z\"}],[\"$\",\"meta\",\"4\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"5\",{\"name\":\"twitter:title\",\"content\":\"강의 요약 - CS224n: Natural Language Processing with Deep Learning\"}],[\"$\",\"link\",\"6\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"$L11\",\"7\",{}]],\"error\":null,\"digest\":\"$undefined\"}\n"])</script><script>self.__next_f.push([1,"e:\"$9:metadata\"\n"])</script></body></html>